import os
import ast
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, SystemMessage
from tools import fetch_onedrive_files

# =============================
# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
# =============================
load_dotenv()

# =============================
# AgentState ã®å®šç¾©
# =============================
class AgentState(BaseModel):
    state: str = ""                                  # ç¾åœ¨ã®çŠ¶æ…‹
    question: str = ""                               # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•

    quantity_files: list = Field(default_factory=list, description="é‡çš„ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§")
    quantity_file_contents: dict = Field(default_factory=dict)

    quality_files: list = Field(default_factory=list, description="è³ªçš„ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§")
    quality_file_contents: dict = Field(default_factory=dict)

    selected_files: list = Field(default_factory=list)  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«

    answer: str = ""           # LLMã®è¿”ç­”ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«é¸æŠï¼‰
    predict_answer: str = ""   # LLMã®æœ€çµ‚åˆ†æçµæœ

    access_token: str          # OneDrive APIç”¨ãƒˆãƒ¼ã‚¯ãƒ³

# =============================
# LLMï¼ˆGoogle Geminiï¼‰
# =============================
llm = ChatGoogleGenerativeAI(
    model=os.getenv("GEMINI_MODEL"),
    temperature=0.5,
    transport="rest"
)

# =============================
# â‘  ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒãƒ¼ãƒ‰
# =============================
def select_file_node(state: AgentState) -> AgentState:
    system_prompt = f"""
    ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿é¸å®šã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¾é ¼å†…å®¹ã«é–¢ä¿‚ã®ã‚ã‚‹ã‚‚ã®ã ã‘ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
    
    âœ… å‡ºåŠ›ãƒ«ãƒ¼ãƒ«ï¼š
    ãƒ»Python ã® list å½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š['finance.csv', 'healthcare.csv']ï¼‰
    ãƒ»æ–‡ç« ã‚„JSONå½¢å¼ã¯ç¦æ­¢ã§ã™

    é¸æŠå¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ï¼š{state.quantity_files}
    """
    messages = [SystemMessage(content=system_prompt),
                HumanMessage(content=state.question)]
    state.answer = llm.invoke(messages).content
    state.state = "file_selected"
    return state

# âœ… ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠçµæœã®å½¢å¼ãŒæ­£ã—ã„ã‹ãƒã‚§ãƒƒã‚¯
def is_list_or_not(state):
    try:
        parsed = ast.literal_eval(state.answer)
        return "list" if isinstance(parsed, list) else "other"
    except Exception:
        return "other"

# =============================
# â‘¡ é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‚’å–å¾—
# =============================
def quantity_files_node(state: AgentState) -> AgentState:
    try:
        selected = ast.literal_eval(state.answer)
        if not selected:
            state.predict_answer = "âš  ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
            state.state = "no_files_selected"
            return state
        state.selected_files = selected
    except Exception:
        state.predict_answer = "âš  ['finance.csv'] ã®ã‚ˆã†ã«ãƒªã‚¹ãƒˆå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        state.state = "error_parsing_list"
        return state

    state.quantity_file_contents = fetch_onedrive_files(
        file_names=state.selected_files,
        access_token=state.access_token
    )
    state.state = "fetched_quantity_files"
    return state

# =============================
# â‘¢ è³ªçš„ãƒ‡ãƒ¼ã‚¿ï¼ˆä»»æ„ãƒ»æœªä½¿ç”¨ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—å¯ï¼‰
# =============================
def quality_files_node(state: AgentState) -> AgentState:
    if not state.quality_files:
        state.state = "skip_quality"
        return state

    state.quality_file_contents = fetch_onedrive_files(
        file_names=state.quality_files,
        access_token=state.access_token,
        folder_path="Test2"
    )
    state.state = "fetched_quality_files"
    return state

# =============================
# â‘£ æœ€çµ‚åˆ†æãƒãƒ¼ãƒ‰
# =============================
def predict_node(state: AgentState) -> AgentState:
    system_prompt = f"""
    ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã™ã€‚
    ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€å®šé‡çš„ãƒ»å®šæ€§çš„ãªåˆ†æã‚’è¡Œã„ã€æ´å¯Ÿã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚

    --- é‡çš„ãƒ‡ãƒ¼ã‚¿ ---
    {state.quantity_file_contents}

    --- è³ªçš„ãƒ‡ãƒ¼ã‚¿ï¼ˆä»»æ„ï¼‰---
    {state.quality_file_contents}

    âœ… å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼š
    ### âœ… ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆäº‹å®Ÿãƒ»å‚¾å‘ï¼‰
    -
    ### ğŸ’¡ ä»®èª¬ãƒ»ç¤ºå”†
    -
    ### âš  ãƒªã‚¹ã‚¯ãƒ»æ‡¸å¿µç‚¹
    -
    ### ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ
    -
    """
    messages = [SystemMessage(content=system_prompt),
                HumanMessage(content=state.question)]
    state.predict_answer = llm.invoke(messages).content
    state.state = "predict_done"
    return state

# =============================
# â‘¤ ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ãƒ‰ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
# =============================
def error_node(state: AgentState) -> AgentState:
    state.state = "error"
    state.predict_answer = "âš  æ­£ã—ã„å½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š['finance.csv']ï¼‰"
    return state

# =============================
# LangGraph æ§‹ç¯‰
# =============================
graph = StateGraph(AgentState)

graph.add_node("select_file_node", select_file_node)
graph.add_node("quantity_files_node", quantity_files_node)
graph.add_node("quality_files_node", quality_files_node)
graph.add_node("predict_node", predict_node)
graph.add_node("error_node", error_node)

graph.add_edge(START, "select_file_node")

graph.add_conditional_edges(
    "select_file_node",
    is_list_or_not,
    {
        "list": "quantity_files_node",
        "other": "error_node"
    }
)

graph.add_edge("quantity_files_node", "quality_files_node")
graph.add_edge("quality_files_node", "predict_node")
graph.add_edge("predict_node", END)
graph.add_edge("error_node", END)

app = graph.compile()